{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7465ac37",
   "metadata": {},
   "source": [
    "# Regularisation with normal method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0e3dbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import inv,pinv,LinAlgError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50c94953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "X,y = datasets.fetch_california_housing(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f83830",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp1 = X[0:16000,:]  #split into train and test\n",
    "X_train = np.ones((X_train_temp1.shape[0],X_train_temp1.shape[1]+1))  #putting extra column for dummy variable\n",
    "X_train[:,1:] = X_train_temp1  #now our X_train is ready with dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ffbd8366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing same as above for test inputs\n",
    "X_test_temp1 = X[16000:,:]\n",
    "X_test = np.ones((X_test_temp1.shape[0],X_test_temp1.shape[1]+1))  \n",
    "X_test[:,1:] = X_test_temp1 \n",
    "\n",
    "y_train=y[:16000]\n",
    "y_test=y[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5be334aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.46673867e+01  4.51612989e-01  9.02993771e-03 -1.26541896e-01\n",
      "  7.28281268e-01 -4.63890213e-06 -8.65767445e-03 -4.04037559e-01\n",
      " -4.10170251e-01]\n"
     ]
    }
   ],
   "source": [
    "# regularisation for normalised method\n",
    "\n",
    "reg_mat =np.eye(X_train.shape[1])\n",
    "reg_mat[0,0]=0\n",
    "lam=0.008\n",
    "\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "XTXi = pinv(np.dot(X_train.T,X_train) + lam*reg_mat) #it returns inverse if possible, else it returns pseudo inverse\n",
    "XTy = np.dot(X_train.T,y_train)\n",
    "\n",
    "theta = np.dot(XTXi,XTy)\n",
    "print(theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d633c4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.5183921609455002\n",
      "MSE:  0.4960637051055179\n"
     ]
    }
   ],
   "source": [
    "#testing the dataset with calculated values of features\n",
    "pred = np.dot(theta,X_test.T)\n",
    "\n",
    "#calculating mae and mse\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))\n",
    "\n",
    "# lam=1200\n",
    "# MAE:  0.5243278595310583\n",
    "# MSE:  0.51354617492903\n",
    "\n",
    "# lam=15\n",
    "# MAE:  0.5183038181677097\n",
    "# MSE:  0.4957636466824587\n",
    "\n",
    "# lam=0.008\n",
    "# MAE:  0.5183921609455002\n",
    "# MSE:  0.4960637051055179"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be174d7",
   "metadata": {},
   "source": [
    "# Regularisation with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21607517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardising inputs\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train[:,1:])\n",
    "X_train[:,1:] = scaler.transform(X_train[:,1:])\n",
    "X_test[:,1:] = scaler.transform(X_test[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67a2f677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.02383675  0.83235539  0.23230785 -0.14734938  0.13279831  0.03188788\n",
      " -0.06189764 -0.09386915 -0.02412557]\n"
     ]
    }
   ],
   "source": [
    "#assume random values of all 9 parameters\n",
    "theta=np.random.uniform(0,1,size=X_train.shape[1])\n",
    "\n",
    "iter=1000\n",
    "alpha=0.01\n",
    "lam=0.008\n",
    "\n",
    "m=X_train.shape[0]\n",
    "n=X_train.shape[1]\n",
    "\n",
    "for i in range(iter):\n",
    "    update = np.zeros(n)\n",
    "    y_pred = np.dot(X_train,theta)\n",
    "    error = y_pred - y_train\n",
    "    \n",
    "    for j in range(n):\n",
    "        update[j] = np.sum(error*(X_train.T)[j])\n",
    "        \n",
    "    reg = 1 - (alpha*lam)/m\n",
    "    theta = theta*reg - (alpha*update)/m\n",
    "    \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ab2235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.6080765548263042\n",
      "MSE:  0.6892461624236268\n"
     ]
    }
   ],
   "source": [
    "#testing the dataset with calculated values of features\n",
    "pred = np.dot(theta,X_test.T)\n",
    "\n",
    "#calculating mae and mse\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))\n",
    "\n",
    "# lam=1000\n",
    "# MAE:  0.5954799765415979\n",
    "# MSE:  0.6805987680642983\n",
    "\n",
    "# lam=15\n",
    "# MAE:  0.5953541175101127\n",
    "# MSE:  0.6662317628150207\n",
    "\n",
    "# lam=0.008\n",
    "# MAE:  0.6080765548263042\n",
    "# MSE:  0.6892461624236268"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c1e24",
   "metadata": {},
   "source": [
    "## Generate random dataset for height, weigh and predict BMI for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32a88168",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy.linalg import inv,pinv,LinAlgError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7a54d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.23606356 95.4100874 ]\n",
      " [ 2.3639906  91.02344654]\n",
      " [ 1.87573988 72.68204616]\n",
      " ...\n",
      " [ 1.89280677 32.03791248]\n",
      " [ 2.1906665  83.41074137]\n",
      " [ 2.13784106 77.30952771]]\n",
      "[62.44715977 16.2877759  20.65769794 ...  8.94234974 17.38078738\n",
      " 16.91539684]\n"
     ]
    }
   ],
   "source": [
    "# let say 5000 dataset generated\n",
    "\n",
    "heights = np.random.uniform(1.2, 2.4, 5000)\n",
    "weights = np.random.uniform(30, 130, 5000)\n",
    "\n",
    "X = np.column_stack((heights, weights))\n",
    "y = weights/(heights ** 2)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a798a25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp1 = X[0:4000,:]  #split into train and test\n",
    "X_train = np.ones((X_train_temp1.shape[0],X_train_temp1.shape[1]+1))  #putting extra column for dummy variable\n",
    "X_train[:,1:] = X_train_temp1  #now our X_train is ready with dummy variable\n",
    "\n",
    "#doing same as above for test inputs\n",
    "X_test_temp1 = X[4000:,:]\n",
    "X_test = np.ones((X_test_temp1.shape[0],X_test_temp1.shape[1]+1))  \n",
    "X_test[:,1:] = X_test_temp1 \n",
    "\n",
    "y_train=y[:4000]\n",
    "y_test=y[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc2be1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3.958876398175509\n",
      "MSE:  27.679062532579266\n"
     ]
    }
   ],
   "source": [
    "# regularisation for normalised method\n",
    "\n",
    "reg_mat =np.eye(X_train.shape[1])\n",
    "reg_mat[0,0]=0\n",
    "lam = 50\n",
    "\n",
    "theta = np.zeros(X_train.shape[1])\n",
    "\n",
    "XTXi = pinv(np.dot(X_train.T,X_train) + lam*reg_mat) #it returns inverse if possible, else it returns pseudo inverse\n",
    "XTy = np.dot(X_train.T,y_train)\n",
    "\n",
    "theta = np.dot(XTXi,XTy)\n",
    "\n",
    "#testing the dataset with calculated values of features\n",
    "pred = np.dot(theta,X_test.T)\n",
    "\n",
    "#calculating mae and mse\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c5b59dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3.85176515331495\n",
      "MSE:  36.70573249692512\n"
     ]
    }
   ],
   "source": [
    "# regularisation for gradient descent method\n",
    "\n",
    "# standardising inputs\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train[:,1:])\n",
    "X_train[:,1:] = scaler.transform(X_train[:,1:])\n",
    "X_test[:,1:] = scaler.transform(X_test[:,1:])\n",
    "\n",
    "theta=np.random.uniform(0,1,size=X_train.shape[1])\n",
    "\n",
    "iter=1000\n",
    "alpha=0.01\n",
    "lam=500\n",
    "\n",
    "m=X_train.shape[0]\n",
    "n=X_train.shape[1]\n",
    "\n",
    "for i in range(iter):\n",
    "    update = np.zeros(n)\n",
    "    y_pred = np.dot(X_train,theta)\n",
    "    error = y_pred - y_train\n",
    "    \n",
    "    for j in range(n):\n",
    "        update[j] = np.sum(error*(X_train.T)[j])\n",
    "        \n",
    "    reg = 1 - (alpha*lam)/m\n",
    "    theta = theta*reg - (alpha*update)/m\n",
    "    \n",
    "#testing the dataset with calculated values of features\n",
    "pred = np.dot(theta,X_test.T)\n",
    "\n",
    "#calculating mae and mse\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade7d04c",
   "metadata": {},
   "source": [
    "# Using sklearn for Lasso, Ridge and SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db489b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb86569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n"
     ]
    }
   ],
   "source": [
    "X,y = datasets.fetch_california_housing(return_X_y=True)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966c7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp1 = X[0:16000,:]  #split into train and test\n",
    "X_train = np.ones((X_train_temp1.shape[0],X_train_temp1.shape[1]+1))  #putting extra column for dummy variable\n",
    "X_train[:,1:] = X_train_temp1  #now our X_train is ready with dummy variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41b853d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doing same as above for test inputs\n",
    "X_test_temp1 = X[16000:,:]\n",
    "X_test = np.ones((X_test_temp1.shape[0],X_test_temp1.shape[1]+1))  \n",
    "X_test[:,1:] = X_test_temp1 \n",
    "\n",
    "y_train=y[:16000]\n",
    "y_test=y[16000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb392521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  4.15189250e-01  9.91595884e-03 -5.85249767e-02\n",
      "  3.75304298e-01 -1.52286979e-06 -8.59595836e-03 -3.85427174e-01\n",
      " -3.84114478e-01]\n",
      "-32.08948159789703\n",
      "MAE:  0.5186931326757339\n",
      "MSE:  0.4953082284897369\n"
     ]
    }
   ],
   "source": [
    "# Lasso used for normalised regularisation\n",
    "# optimizing cost fxn + lambda*theta1^2 +..+ lambda*thetan^2\n",
    "h = 0.01 #lambda\n",
    "lr = Lasso(alpha = h)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a655610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  4.51612696e-01  9.02994274e-03 -1.26541356e-01\n",
      "  7.28278457e-01 -4.63888403e-06 -8.65767604e-03 -4.04037523e-01\n",
      " -4.10170166e-01]\n",
      "-34.66737681723917\n",
      "MAE:  0.5183921482479996\n",
      "MSE:  0.4960636616962555\n"
     ]
    }
   ],
   "source": [
    "# Ridge used for normalised regularisation\n",
    "# optimizing cost fxn + lambda*|theta1| +..+ lambda*|thetan|\n",
    "h = 0.01 #lambda\n",
    "lr = Ridge(alpha = h)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d9df64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardising inputs\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train[:,1:])\n",
    "X_train[:,1:] = scaler.transform(X_train[:,1:])\n",
    "X_test[:,1:] = scaler.transform(X_test[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67f01194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94393832  0.81589169  0.16383716 -0.19845777  0.22862538  0.00926825\n",
      " -0.04235754 -0.52179758 -0.4556087 ]\n",
      "[1.07897104]\n",
      "MAE:  0.5318946351483788\n",
      "MSE:  0.5305634946326723\n"
     ]
    }
   ],
   "source": [
    "# Ridge used for normalised gradient descent\n",
    "# optimizing cost fxn + lambda*theta1^2 +..+ lambda*thetan^2\n",
    "\n",
    "h = 0.009 #lambda\n",
    "\n",
    "lr = SGDRegressor(eta0 = 0.001, max_iter=1200, alpha = h)\n",
    "lr.fit(X_train,y_train)\n",
    "print(lr.coef_)\n",
    "print(lr.intercept_)\n",
    "pred = lr.predict(X_test)\n",
    "print(\"MAE: \",metrics.mean_absolute_error(y_test,pred))\n",
    "print(\"MSE: \",metrics.mean_squared_error(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dead96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
